{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets with Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will cover the basics of analysing sentiment of tweets using supervised learning. Supervised learning means that a labelled dataset will be used to train our model. We can then test the model and see how accurate it has managed to predict an output. The dataset will first be cleaned, and three different methods of training will be explored: **logistic regression**, which uses a logistic function (Swaminathan); an **LSTM**, which uses a recurrent neural network (Srivastava); and a **transformer**, which uses a pre-trained model (huggingface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the required libraries\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "import io\n",
    "\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification, BertTokenizer\n",
    "from pymagnitude import *\n",
    "from tqdm import tqdm, trange\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset on US airline sentiments downloaded from Kaggle to train our models. The data is orignially from _Crowdflower's Data for Everyone library_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root = '/home/kyubin/Supervised_learning_notebook'\n",
    "data_file = os.path.join(data_root, 'Tweets.csv') \n",
    "\n",
    "tweets_raw = pd.read_csv(data_file) #Importing tweets into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at few of the tweets and their sentiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Do less please @JetBlue, \n",
      "Sentiment: negative\n",
      "Tweet: @USAirways who would like to watch the video of how loyal customers are really treated while asking for an explanation for our 6 hour delay?, \n",
      "Sentiment: negative\n",
      "Tweet: @SouthwestAir FJBFSC  all I need is receipt showing the 776.20 charge I have on my Amex not the 656.xx that I keep getting sent, \n",
      "Sentiment: negative\n",
      "Tweet: @JetBlue what's up w flt 4? Brothers fiancé sitting on board for 30mins w tech  issues., \n",
      "Sentiment: negative\n",
      "Tweet: @AmericanAir There was no one from #AA at the #woase2015 event @HelsinkiAirport\n",
      "Lots of info available of #winterops, \n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    x = random.randint(0, len(tweets_raw))\n",
    "    print('Tweet: %s, \\nSentiment: %s' % (tweets_raw['text'][x], tweets_raw['airline_sentiment'][x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have three different possible outcomes (positive, negative, neutral), it is easier to use numerical values, and not words. We will change the sentiments to numerical values: 0 for negative, 1 for neutral and 2 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing sentiments into numerical values values\n",
    "def sentiment(x):\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    elif x == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "tweets_raw['sentiment'] = tweets_raw['airline_sentiment'].apply(lambda x: sentiment(x))\n",
    "tweets = tweets_raw[['text', 'sentiment']] #Only keeping the useful columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14427 total tweets\n"
     ]
    }
   ],
   "source": [
    "#Cleaning tweets\n",
    "tweets = tweets.drop_duplicates(subset = 'text', keep = 'first')\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"<[^>]*>\", \"\",text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    tweet = text.strip()\n",
    "    return tweet\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(lambda x: preprocessor(x))\n",
    "print('%d total tweets' % (len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets are now all loaded and cleaned! We can now move on to preprocessing, training and testing. We will start with the simplest model, the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, Training, and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, a logistic regression returns a **probability that a value belongs to a certain class**. The simplest form of a logistic regression is called a binary logistic regression, which predicts responses with two possible outcomes. Since we have 3 different outcomes, we have to use a **Multinomial Logistic Regression** (Swaminathan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement** <br>\n",
    "A lot of the code was taken and adapted from Jason Liu's _How can we predict the sentiment by Tweets?_ from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to be preprocessed before it can be used for training. This can be one in many different ways, and the bag-of words model would be the simplest. We will be using Google's Universal Sentence Encoder (Cer et al.), like we did in unsupervised learning, to convert the tweets into vectors. Since the Universal Sentence Encoder can also store the sentiment of the words, it will yield better results that the bag-of-words model, which only stores the numbers of different tokens in a specific tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlodas the pre-trained model from the interent (~1 GB). This may take up to a few minutes.\n",
    "embed = hub.Module('https://tfhub.dev/google/universal-sentence-encoder/1') #(Google)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run([tf.global_variables_initializer(), tf.tables_initializer()]);  # Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "#Putting tweets and sentiment values into separate lists\n",
    "xval = tweets.text.values\n",
    "yval = tweets.sentiment.values\n",
    "\n",
    "#Converting tweets into vectors\n",
    "vecs = sess.run(embed(xval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the data into two groups: training, and testing. We will use 80 percent of the tweets for training, and the rest for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting tweets into training/testing groups\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(vecs, yval, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the tweets vectorized, and split into groups, we can load the model. More details on the parameters can be found in _Logistic Regression_ on Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyubin/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'liblinear', max_iter = 100, C = 0.00001) #Loading model\n",
    "fit = model.fit(xtrain, ytrain) #Training model to our training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now trained, so we can check the accuracy of the model with our testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.6275121275121275\n"
     ]
    }
   ],
   "source": [
    "pred = fit.predict(xtest)\n",
    "accuracy = accuracy_score(pred,ytest)\n",
    "print('The accuracy is: %s' % (float(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>Play around with the parameters; how does the accuracy change as you change the parameters?</p>\n",
    "    <p>Try changing the sizes of the training/testing sets. What would the advantages be of having a bigger training/testing data set?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression simply uses a function to model the dependent and independent variables. Now, we will take a look at a more complex model, where we will have to train our own neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM, or **Long Short-Term Memory**, has a similar architechture to an RNN, which conains loops, allowing previous information to persist. However, an RNN can only store information for short periods of time, and an LSTM can overcome this problem. An LSTM can choose which information to remember and which information to forget, allowing it to remember important imformation for longer periods of time (Srivastava)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement** <br>\n",
    "A lot of code here was taken and adapted from Abhishek Thakur's _Approaching (Almost) Any Problem on Kaggle_. The exact documentation of Keras can be found on keras.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the Universal Sentence Encoder for the LSTM model. Therefore, we will use a pre-trained word embedding model. We will use the GloVe word vectors from Stanford with 300 dimensions (Plasticity Inc). The vectors will be used later on in the section when we create an embedding matrix of all the words. But first, let's import the vectors and get the samples into the right form!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the word vectors from \n",
    "#https://github.com/plasticityai/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models \n",
    "#and import them!\n",
    "vectors = Magnitude(\"/home/kyubin/glove.840B.300d.magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use an LSTM model, we have to tokenize the samples, and turn them into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer(num_words = None) #using a keras tokenizer\n",
    "\n",
    "token.fit_on_texts(tweets.text.values) #List of texts to train on\n",
    "x_seq = token.texts_to_sequences(tweets.text.values) #Turns the tweets into sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets are now all converted into sequences, but they are all of different length. Therefore, we will use a Keras API to **truncade/pad** all the samples into the same length; in this case, 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "x_pad = sequence.pad_sequences(x_seq, maxlen=max_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model to work, all the labels have to be put into **binary form**. The negative sentiments will become [1, 0, 0], the neutral sentiments will become <br> [0, 1, 0], and the positive sentiments will become [0, 0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enc = np_utils.to_categorical(tweets.sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be split into groups. This time, we will also include a validation set, which is important to check whether overfitting is happening (Brownlee). We will use 10 percent of the tweets for testing, and the rest (90 percent) for training and validating. From this 90 percent, 10 percent will be used for validating, and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_val, xtest, ytrain_val, ytest = train_test_split(x_pad, \n",
    "                                                        y_enc, \n",
    "                                                        random_state = 42, \n",
    "                                                        test_size = 0.1, \n",
    "                                                        shuffle = True)\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain_val,\n",
    "                                              ytrain_val, \n",
    "                                              random_state = 42, \n",
    "                                              test_size = 0.1,\n",
    "                                              shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create an embedding matrix for all the words we have in the dataset. The GloVe word vectors will be utilized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15088/15088 [00:17<00:00, 855.83it/s]\n"
     ]
    }
   ],
   "source": [
    "word_index = token.word_index #A dictionary of all the words and the corresponding values\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = vectors.query(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we finally have all our data ready for training and testing, we can intialize the model. We will create a neural network with a GloVe embedding layer, an LSTM layer, and two dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() #this model is a linear stack of layers\n",
    "#Embedding Layer - turns positive integers into dense vectors of fized size\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "#LSTM Layer\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "#Dense layer 1 (just a regular densely - connected NN layer)\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "#Dense layer 2\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3)) #Final output dimensionality of 3, because we have three possible outcomes!\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>On lines 8, 13, and 16 above, there are dropout layers included in the model. A dropout layer sets a fraction of the inputs to zero at each update during training time. Why is this important? What does it prevent?</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kyubin/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11685 samples, validate on 1299 samples\n",
      "Epoch 1/20\n",
      "11685/11685 [==============================] - 7s 577us/step - loss: 0.9075 - acc: 0.6169 - val_loss: 0.7772 - val_acc: 0.6528\n",
      "Epoch 2/20\n",
      "11685/11685 [==============================] - 5s 410us/step - loss: 0.7810 - acc: 0.6347 - val_loss: 0.7010 - val_acc: 0.6990\n",
      "Epoch 3/20\n",
      "11685/11685 [==============================] - 5s 403us/step - loss: 0.7324 - acc: 0.6909 - val_loss: 0.6363 - val_acc: 0.7598\n",
      "Epoch 4/20\n",
      "11685/11685 [==============================] - 5s 410us/step - loss: 0.6830 - acc: 0.7247 - val_loss: 0.5813 - val_acc: 0.7737\n",
      "Epoch 5/20\n",
      "11685/11685 [==============================] - 5s 399us/step - loss: 0.6531 - acc: 0.7386 - val_loss: 0.5512 - val_acc: 0.7814\n",
      "Epoch 6/20\n",
      "11685/11685 [==============================] - 5s 398us/step - loss: 0.6429 - acc: 0.7408 - val_loss: 0.5363 - val_acc: 0.7975\n",
      "Epoch 7/20\n",
      "11685/11685 [==============================] - 5s 404us/step - loss: 0.6362 - acc: 0.7478 - val_loss: 0.5353 - val_acc: 0.7952\n",
      "Epoch 8/20\n",
      "11685/11685 [==============================] - 5s 404us/step - loss: 0.6260 - acc: 0.7499 - val_loss: 0.5400 - val_acc: 0.7975\n",
      "Epoch 9/20\n",
      "11685/11685 [==============================] - 5s 397us/step - loss: 0.6201 - acc: 0.7528 - val_loss: 0.5215 - val_acc: 0.7998\n",
      "Epoch 10/20\n",
      "11685/11685 [==============================] - 5s 401us/step - loss: 0.6126 - acc: 0.7558 - val_loss: 0.5240 - val_acc: 0.7975\n",
      "Epoch 11/20\n",
      "11685/11685 [==============================] - 4s 381us/step - loss: 0.6143 - acc: 0.7532 - val_loss: 0.5242 - val_acc: 0.7945\n",
      "Epoch 12/20\n",
      "11685/11685 [==============================] - 5s 388us/step - loss: 0.5994 - acc: 0.7546 - val_loss: 0.5170 - val_acc: 0.7998\n",
      "Epoch 13/20\n",
      "11685/11685 [==============================] - 5s 392us/step - loss: 0.6062 - acc: 0.7565 - val_loss: 0.5090 - val_acc: 0.7998\n",
      "Epoch 14/20\n",
      "11685/11685 [==============================] - 5s 396us/step - loss: 0.5955 - acc: 0.7588 - val_loss: 0.5140 - val_acc: 0.8006\n",
      "Epoch 15/20\n",
      "11685/11685 [==============================] - 4s 372us/step - loss: 0.5945 - acc: 0.7623 - val_loss: 0.5118 - val_acc: 0.8037\n",
      "Epoch 16/20\n",
      "11685/11685 [==============================] - 4s 370us/step - loss: 0.5876 - acc: 0.7676 - val_loss: 0.5006 - val_acc: 0.8052\n",
      "Epoch 17/20\n",
      "11685/11685 [==============================] - 4s 370us/step - loss: 0.5923 - acc: 0.7647 - val_loss: 0.5012 - val_acc: 0.8029\n",
      "Epoch 18/20\n",
      "11685/11685 [==============================] - 4s 369us/step - loss: 0.5849 - acc: 0.7612 - val_loss: 0.5210 - val_acc: 0.7968\n",
      "Epoch 19/20\n",
      "11685/11685 [==============================] - 4s 369us/step - loss: 0.5876 - acc: 0.7659 - val_loss: 0.4972 - val_acc: 0.8022\n",
      "Epoch 20/20\n",
      "11685/11685 [==============================] - 4s 371us/step - loss: 0.5779 - acc: 0.7674 - val_loss: 0.4945 - val_acc: 0.8075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a842e9160>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model with the training dataset\n",
    "model.fit(xtrain, ytrain, batch_size=512, epochs=20, verbose=1, validation_data = (xval, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this model performs on our testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 0.5234636724697411\n",
      "The accuracy is 0.7796257771887221\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "loss, acc = model.evaluate(x = xtest, y = ytest, verbose = 2, batch_size = 512)\n",
    "print('The loss is %s' % (loss))\n",
    "print('The accuracy is %s' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>Play around with the parameters again! Which parameters affects the results the most?</p>\n",
    "    <p>Using the training and the validation accuracy values, how would we see if overfitting is happening?</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we looked at how we can train our own neural network. To improve our model, we would have to gather and label more data, add more layers, adjust parameters, etc. However, this is very time consuming and expensive. Therefore, in the next section, we will take a look at how we can use a model that has already been trained with billions of data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement**<br>\n",
    "A lot of the code in this section was taken and adapted from Chris McCormick's _BERT Fine-Tuning Tutorial with PyTorch_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model we will explore is the BERT transformer model. BERT is a pre-trained deep learning model, which has been trained with billions of data samples. To use this model, we have to fine-tune it, so it suits our specific task. Here are a few reasons why fine tuning a model is better than building and training a model from scratch (McCormick).<br>\n",
    "1. It is a lot easier to fine tune a model, as it contains a lot of information already.\n",
    "2. Saves time & money\n",
    "3. Less data required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>Can you think of other reasons why fine tuning a model would be better than building and training a model from scratch?</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning a model also requires a lot of computing power. Therefore, it is recommended to use a GPU. In order to use a GPU in pytorch, we need to specifiy it as the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN V'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fine-tune the model, the data has to be in the same form as the data which was used to pre-train the orignial model. Therefore, we will use the BERT tokenizer to tokenize the data. Further, in order for BERT to work properly, special tokens must be added at the beginning and the end of each sentence. [CLS] must be added to the beginning, and [SEP] to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tweets.text.values #Making a list of all the tweets\n",
    "\n",
    "texts = [\"[CLS] \" + text + \" [SEP]\" for text in texts] #Adding special tokens to beginning/end of tweets\n",
    "labels = tweets.sentiment.values #List of sentiment values\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) #Applying tokenizer to all tweets\n",
    "tokenized_texts = [tokenizer.tokenize(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at how the tokenized tweets look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet tokenized: ['[CLS]', 'virgin', '##ame', '##rica', 'what', 'dh', '##ep', '##burn', 'said', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print (\"First tweet tokenized:\", tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are tokenized into words now, but words cannot be inputed directly into our model. Therefore, we will convert our tokens to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] #Converts tokens to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with the LSTM model, we have to get all our samples to be the same length, and this is done by padding the data. We will make the length 30 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") \n",
    "#Padding and truncating both at the end of the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will use 81 percent of the data for training, 9 percent for validating, and 10 percent for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_val, xtest, ytrain_val, ytest = train_test_split(input_ids, \n",
    "                                                        labels, \n",
    "                                                        random_state = 42, \n",
    "                                                        test_size = 0.1, \n",
    "                                                        shuffle = True)\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain_val,\n",
    "                                              ytrain_val, \n",
    "                                              random_state = 42, \n",
    "                                              test_size = 0.1,\n",
    "                                              shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting numpy arrays into torch tensors\n",
    "xtrain = torch.tensor(xtrain)\n",
    "xval = torch.tensor(xval)\n",
    "xtest = torch.tensor(xtest) #Will be used in the testing section\n",
    "ytrain = torch.tensor(ytrain)\n",
    "yval = torch.tensor(yval)\n",
    "ytest = torch.tensor(ytest) #Will be used in the testing section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save memory during training, we will create an iterator of our data with torch DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training (16 or 32 recommended)\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(xtrain, ytrain)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(xval, yval)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(xtest, ytest)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now get the training parameters from within the BERT trained model. These parameters will get updated during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta'] \n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01}, #Parameters where the weights will be decayed\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0} #Parameters where the weights will be fixed\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5, #learning rate\n",
    "                     warmup = 0.1) #Proportion of total training steps that is used as a warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below calculates the accuracy of the prediction compared to the given labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the training! It will be divided into two section, the training section, and the validation section. We will be using 3 epochs, but this can be varied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>The more epochs you have, the higher training accuracy you would get. However, having too many epochs is not a good idea. Why?</p>\n",
    "    \n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5691460691056075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 1/3 [01:35<03:10, 95.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8376524390243902\n",
      "Training loss: 0.32340468835243613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 2/3 [03:08<01:34, 94.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8475609756097561\n",
      "Training loss: 0.1877086680865351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [04:40<00:00, 93.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8414634146341463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for _ in trange(epochs, desc=\"Epoch\"): \n",
    "  # Trainingop\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
    "    b_input_ids, b_labels = batch # Unpack the inputs from our dataloader\n",
    "    optimizer.zero_grad() # Clear out the gradients (by default they accumulate)\n",
    "    loss = model(b_input_ids, labels=b_labels) # Forward propagation\n",
    "    loss.backward() # Backward propagation\n",
    "    optimizer.step() # Update parameters and take a step using the computed gradient\n",
    "\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Training loss: %s\" % (tr_loss/nb_tr_steps))\n",
    "    \n",
    "  # Validation\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
    "    b_input_ids, b_labels = batch # Unpack the inputs from our dataloader\n",
    "    with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory & speeding up validation\n",
    "      logits = model(b_input_ids) # Forward pass, calculate logit predictions\n",
    "    logits = logits.detach().cpu().numpy() # Move logits and labels to CPU\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: %s\" % (eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the testing dataset we created earlier to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy: 0.8337912087912088\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
    "    b_input_ids, b_labels = batch # Unpack the inputs from our dataloader\n",
    "    with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory & speeding up validation\n",
    "      logits = model(b_input_ids) # Forward pass, calculate logit predictions\n",
    "    logits = logits.detach().cpu().numpy() # Move logits and labels to CPU\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"Test set Accuracy: %s\" % (eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Question</h3>\n",
    "    <p>Compare the accuracy values of all the models! Do the results make sense?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brownlee, Jason. “What Is the Difference Between Test and Validation Datasets?” *Machine Learning Mastery*, 26 July 2017, machinelearningmastery.com/difference-test-validation-datasets/. Accessed 11 Sept. 2019.\n",
    "\n",
    "Cer, Daniel, et al. _Universal Sentence Encoder_. 2018 Feb. 2018.\n",
    "\n",
    "Google. “TensorFlow Hub.” *Tfhub.Dev*, Google, 2019, tfhub.dev/google/universal-sentence-encoder/1. Accessed 30 Aug. 2019.\n",
    "\n",
    "google-research. “BERT.” *GitHub*, 28 Mar. 2019, github.com/google-research/bert.\n",
    "\n",
    "huggingface. “Huggingface/Pytorch-Transformers.” *GitHub*, 11 Sept. 2019, github.com/huggingface/pytorch-transformers#quick-tour. Accessed 11 Sept. 2019.\n",
    "\n",
    "Keras. “Keras: The Python Deep Learning Library.” *Keras Documentation*, 2019, keras.io/. Accessed 11 Sept. 2019.\n",
    "\n",
    "Liu, Jason. “How Can We Predict the Sentiment by Tweets?” *Kaggle*, 18 Oct. 2016, www.kaggle.com/jiashenliu/how-can-we-predict-the-sentiment-by-tweets/notebook. Accessed 11 Sept. 2019.\n",
    "\n",
    "McCormick, Chris. “BERT Fine-Tuning Tutorial with PyTorch.” *Chris McCormick*, 22 July 2019, mccormickml.com/2019/07/22/BERT-fine-tuning/. Accessed 11 Sept. 2019.\n",
    "\n",
    "Plasticity Inc. “Magnitude: A Fast, Simple Vector Embedding Utility Library.” *GitHub*, 28 Nov. 2018, github.com/plasticityai/magnitude. Accessed 30 Aug. 2019.\n",
    "\n",
    "Scikit-Learn. “Logistic Regression.” *Scikit-Learn.Org*, 2014, scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html. Accessed 11 Sept. 2019.\n",
    "\n",
    "Srivastava, Pranjal. “Essentials of Deep Learning : Introduction to Long Short Term Memory.” *Analytics Vidhya*, 23 Dec. 2017, www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/. Accessed 11 Sept. 2019.\n",
    "\n",
    "Swaminathan, Saishruthi. “Logistic Regression — Detailed Overview.” *Medium*, Towards Data Science, 15 Mar. 2018, towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc. Accessed 11 Sept. 2019.\n",
    "\n",
    "Thakur, Abhishek. “Approaching (Almost) Any NLP Problem on Kaggle.” *Kaggle*, 24 July 2018, www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle/comments. Accessed 11 Sept. 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
